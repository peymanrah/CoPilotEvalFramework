# OpenAI API Integrated UX Evaluation Framework

## Technical Implementation Guide for AI Systems

---

**Document Version:** 1.0  
**Date:** January 13, 2026  
**Classification:** Microsoft Internal  
**Authors:** Evaluation Framework Team  

---

## Executive Summary

### Purpose and Scope

This document provides a comprehensive technical specification for the **OpenAI API Integrated UX Evaluation Framework**â€”a novel, scalable approach to benchmarking Microsoft Copilot against competitors (ChatGPT, Gemini) using automated browser agents and LLM-as-judge evaluation.

**Key Objectives:**
- Identify Microsoft Copilot capability gaps through competitive analysis
- Scale user feedback validation from manual OCV surveys to automated synthetic testing
- Provide actionable insights to product teams using both textual and visual response evaluation

### Proof-of-Concept Results Summary

| Metric | Value |
|--------|-------|
| Total Synthetic Prompts Generated | 2,640 |
| Prompts Evaluated (POC Run) | 100 |
| Chatbots Tested | 3 (Copilot, ChatGPT, Gemini) |
| Total Evaluations | 300 (100 Ã— 3 chatbots) |
| Successful Responses | 203 (67.7%) |
| Screenshots Captured | 645 |
| Low Satisfaction Intents Identified | 20+ (0% sat_rate) |
| Intent Categories from OCV Data | 73 |

### Key Benefits

1. **Automated Competitive Intelligence** â€” Continuous monitoring of Copilot vs. ChatGPT vs. Gemini
2. **Data-Driven Prioritization** â€” Focus engineering on intents with lowest user satisfaction
3. **Visual + Textual Analysis** â€” Novel dual-mode evaluation for comprehensive UX assessment
4. **Scalable Architecture** â€” Designed to run thousands of synthetic prompts across multiple chatbots

---

## Table of Contents

1. [Framework Overview](#1-framework-overview)
2. [Evaluation Dimensions](#2-evaluation-dimensions)
3. [Scoring and Aggregation Formulas](#3-scoring-and-aggregation-formulas)
4. [OpenAI API Integration](#4-openai-api-integration)
5. [Implementation Workflow](#5-implementation-workflow)
6. [Low Satisfaction Intent Analysis](#6-low-satisfaction-intent-analysis)
7. [Competitive Benchmark Results](#7-competitive-benchmark-results)
8. [Screenshot-Based Visual Evaluation](#8-screenshot-based-visual-evaluation)
9. [Text Response Evaluation Framework](#9-text-response-evaluation-framework)
10. [Bias Mitigation and Calibration](#10-bias-mitigation-and-calibration)
11. [Reporting and Visualization](#11-reporting-and-visualization)
12. [Appendix: Statistical Notes](#12-appendix-statistical-notes)

---

## 1. Framework Overview

### 1.1 Goals

The UX Evaluation Framework is designed to achieve three core objectives:

| Goal | Description |
|------|-------------|
| **Reliability** | Consistent, reproducible evaluations across runs using deterministic prompts and standardized evaluation criteria |
| **Reproducibility** | Full traceability from OCV user feedback â†’ synthetic prompts â†’ chatbot responses â†’ evaluation scores |
| **Scalability** | Architecture supports evaluation of 2,640+ synthetic prompts across multiple chatbots with automated browser agents |

### 1.2 Design Principles

#### Automated LLM-as-Judge Evaluation
Instead of manual human review, the framework uses OpenAI's GPT-4 as an automated judge to score responses across multiple dimensions. This enables:
- Consistent scoring criteria applied uniformly
- 24/7 evaluation capability without human fatigue
- Detailed reasoning for each score

#### Multi-Dimensional UX Metrics
Each response is evaluated across 7+ dimensions (see Section 2) rather than a single quality score, providing granular insights into specific capability gaps.

#### Stealth Browser Automation
The `chatbot_stealth_agent.py` component uses Playwright with stealth techniques to:
- Avoid bot detection across Copilot, ChatGPT, and Gemini
- Capture full response text and visual screenshots
- Handle dynamic content loading and infinite scroll

```python
# From chatbot_stealth_agent.py - Stealth browser configuration
self.browser = await playwright.chromium.launch(
    headless=False,
    args=[
        '--disable-blink-features=AutomationControlled',
        '--disable-dev-shm-usage',
        '--no-sandbox',
    ]
)
```

### 1.3 Architecture Advantages

| Advantage | Implementation |
|-----------|----------------|
| **Continuous Monitoring** | Scheduled runs detect Copilot regressions vs. competitors |
| **Reduced Manual Effort** | Automated collection replaces manual testing (~15 min/prompt â†’ 30 sec/prompt) |
| **Dual-Mode Capture** | Both text responses and visual screenshots enable comprehensive analysis |
| **Competitive Benchmarking** | Same prompt tested across 3 chatbots in single run |

### 1.4 POC Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        OCV User Feedback Data                                â”‚
â”‚                    (BizchatIntent.csv - 73 intents)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Synthetic Prompt Generator                                â”‚
â”‚            (synthetic_prompt_generator.py + add_context_to_prompts_v4.py)   â”‚
â”‚                                                                              â”‚
â”‚  â€¢ Identifies low satisfaction intents (0% sat_rate)                        â”‚
â”‚  â€¢ Generates 2,640 synthetic prompts with context URLs                      â”‚
â”‚  â€¢ Complexity levels: simple, medium, complex, expert                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Stealth Browser Agent                                    â”‚
â”‚                    (chatbot_stealth_agent.py)                               â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚   Copilot   â”‚   â”‚   ChatGPT   â”‚   â”‚   Gemini    â”‚                       â”‚
â”‚  â”‚ copilot.    â”‚   â”‚ chatgpt.com â”‚   â”‚ gemini.     â”‚                       â”‚
â”‚  â”‚ microsoft.  â”‚   â”‚             â”‚   â”‚ google.com  â”‚                       â”‚
â”‚  â”‚ com         â”‚   â”‚             â”‚   â”‚             â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚        â”‚                 â”‚                 â”‚                                â”‚
â”‚        â–¼                 â–¼                 â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Capture: Full Text Response + Visual Screenshots           â”‚           â”‚
â”‚  â”‚  POC Stats: 203/300 successful, 645 screenshots             â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Evaluation & Scoring Layer                                â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚  OpenAI GPT-4 Judge  â”‚    â”‚  Vision Model Judge  â”‚                       â”‚
â”‚  â”‚  (Text Evaluation)   â”‚    â”‚  (Screenshot Eval)   â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                              â”‚
â”‚  Output: evaluation_consolidated.csv with per-chatbot scores                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Evaluation Dimensions

### 2.1 Dimension Definitions

The framework evaluates chatbot responses across seven core dimensions, each targeting a specific aspect of user experience:

| Dimension | Description | Weight | Scoring Range |
|-----------|-------------|--------|---------------|
| **Factuality** | Accuracy of claims and information provided. Responses must be grounded in the provided context or verifiable facts. | 0.20 | 1-5 |
| **Helpfulness** | Degree to which the response addresses the user's actual need. Measures task completion and actionability. | 0.20 | 1-5 |
| **Safety** | Absence of harmful, biased, or inappropriate content. Includes PII protection and professional tone. | 0.15 | 1-5 |
| **Robustness** | Consistency of quality across prompt variations. Same intent phrased differently should yield comparable responses. | 0.10 | 1-5 |
| **Latency** | Response time from prompt submission to completion. Measured in seconds. | 0.10 | 1-5 (scaled) |
| **Formatting** | Visual quality, structure, and readability. Includes proper use of headers, lists, tables, and code blocks. | 0.15 | 1-5 |
| **Memory** | Ability to maintain context across conversation turns (for multi-turn evaluations). | 0.10 | 1-5 |

### 2.2 Dimension-Specific Evaluation Criteria

#### Factuality Scoring Rubric

| Score | Criteria |
|-------|----------|
| 5 | All claims verifiable from context; no hallucinations; proper citations |
| 4 | Minor unsupported claims; core facts accurate |
| 3 | Mix of accurate and questionable claims; some grounding issues |
| 2 | Significant factual errors; poor grounding |
| 1 | Predominantly hallucinated or fabricated content |

#### Helpfulness Scoring Rubric

| Score | Criteria |
|-------|----------|
| 5 | Fully addresses user intent; actionable; complete |
| 4 | Addresses most needs; minor gaps in completeness |
| 3 | Partially helpful; requires follow-up clarification |
| 2 | Minimally helpful; misses key aspects of the request |
| 1 | Does not address user need; irrelevant response |

#### Formatting Scoring Rubric (Visual Assessment)

| Score | Criteria |
|-------|----------|
| 5 | Excellent structure; appropriate headers/lists/tables; scannable |
| 4 | Good structure; minor formatting improvements possible |
| 3 | Adequate formatting; some walls of text or missing structure |
| 2 | Poor formatting; difficult to scan or parse |
| 1 | No formatting; unstructured text block |

### 2.3 POC Evaluation Dimensions Applied

From `synthetic_prompt_generator.py`, the POC generates prompts with these evaluation dimensions:

```python
class EvaluationDimension(Enum):
    """Dimensions for LLM-as-judge evaluation"""
    ACCURACY = "accuracy"
    COMPLETENESS = "completeness"
    RELEVANCE = "relevance"
    CLARITY = "clarity"
    ACTIONABILITY = "actionability"
    HELPFULNESS = "helpfulness"
    SAFETY = "safety"
    CONSISTENCY = "consistency"
```

Each synthetic prompt in `synthetic_prompts_v4_enhanced_*.csv` includes its applicable dimensions:

```csv
evaluation_dimensions
["accuracy", "clarity", "strategic_relevance", "conciseness", "professionalism"]
```

---

## 3. Scoring and Aggregation Formulas

### 3.1 Dimension-Level Scoring

Each dimension receives a raw score from the LLM judge on a 1-5 scale. The prompt template for scoring:

```
You are an expert evaluator assessing AI assistant responses.

PROMPT: {synthetic_prompt}
CONTEXT: {context_text}
RESPONSE: {chatbot_response}

Score this response on {dimension} using a 1-5 scale:
5 = Excellent, 4 = Good, 3 = Adequate, 2 = Poor, 1 = Very Poor

Provide:
1. Score (integer 1-5)
2. Reasoning (2-3 sentences)
```

### 3.2 Trimmed Median Aggregation

To reduce the impact of outlier scores (from ambiguous prompts or edge cases), we apply **trimmed median** aggregation:

$$
\text{TrimmedMedian}(S, p) = \text{Median}\left(S \setminus \{S_{[1..k]}, S_{[n-k+1..n]}\}\right)
$$

Where:
- $S$ = sorted list of dimension scores
- $p$ = trim percentage (default: 10%)
- $k = \lfloor n \times p \rfloor$ = number of scores trimmed from each end

**Example:** For 100 scores per dimension, trim top/bottom 10 scores, then take median of remaining 80.

### 3.3 Weighted Geometric Mean Formula

The **Overall UX Score** combines dimension scores using a weighted geometric mean, which:
- Penalizes poor performance in any dimension (multiplicative)
- Prevents high scores in one area from masking deficiencies

$$
\text{UX}_{\text{overall}} = \left( \prod_{i=1}^{n} D_i^{w_i} \right)^{\frac{1}{\sum w_i}}
$$

Where:
- $D_i$ = normalized score for dimension $i$ (scaled 0-1)
- $w_i$ = weight for dimension $i$ (from Section 2.1)

**Expanded formula with POC weights:**

$$
\text{UX}_{\text{overall}} = \left( D_{\text{fact}}^{0.20} \times D_{\text{help}}^{0.20} \times D_{\text{safe}}^{0.15} \times D_{\text{robust}}^{0.10} \times D_{\text{latency}}^{0.10} \times D_{\text{format}}^{0.15} \times D_{\text{memory}}^{0.10} \right)^{1.0}
$$

### 3.4 Latency Normalization

Response time is converted to a 1-5 score using inverse scaling:

$$
D_{\text{latency}} = \max\left(1, 5 - \frac{t - t_{\text{min}}}{t_{\text{max}} - t_{\text{min}}} \times 4\right)
$$

**POC Latency Statistics (from evaluation_consolidated.csv):**

| Chatbot | Avg (sec) | Min (sec) | Max (sec) |
|---------|-----------|-----------|-----------|
| **Copilot** | 14.99 | 0.24 | 61.60 |
| **ChatGPT** | 0.57 | 0.00 | 9.83 |
| **Gemini** | 8.67 | 3.92 | 18.20 |

### 3.5 Score Interpretation Guide

| Overall UX Score | Interpretation | Action Required |
|------------------|----------------|-----------------|
| 4.5 - 5.0 | Excellent | Maintain current quality |
| 4.0 - 4.4 | Good | Minor improvements optional |
| 3.5 - 3.9 | Adequate | Targeted improvements recommended |
| 3.0 - 3.4 | Below Expectations | Priority fixes required |
| < 3.0 | Critical | Immediate intervention needed |

---

## 4. OpenAI API Integration

### 4.1 API Endpoints for Evaluation Tasks

The framework leverages two primary OpenAI API endpoints:

| Endpoint | Purpose | Model |
|----------|---------|-------|
| **Chat Completions** | LLM-as-judge scoring of text responses | GPT-4-turbo |
| **Embeddings** | Semantic similarity for robustness testing | text-embedding-3-large |

### 4.2 Chat Completions for Response Evaluation

The Chat Completions API powers the automated judging system. Each chatbot response is evaluated independently.

```python
import openai
from typing import Dict, List

class ResponseEvaluator:
    """LLM-as-Judge evaluator using OpenAI Chat Completions."""
    
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = "gpt-4-turbo"
    
    async def evaluate_response(
        self,
        prompt: str,
        context: str,
        response: str,
        dimensions: List[str]
    ) -> Dict[str, Dict]:
        """
        Evaluate a chatbot response across multiple dimensions.
        
        Args:
            prompt: The synthetic prompt sent to the chatbot
            context: Reference context (URL or document text)
            response: The chatbot's response text
            dimensions: List of dimensions to evaluate
        
        Returns:
            Dict with scores and reasoning per dimension
        """
        results = {}
        
        for dimension in dimensions:
            evaluation = await self._evaluate_dimension(
                prompt, context, response, dimension
            )
            results[dimension] = evaluation
        
        return results
    
    async def _evaluate_dimension(
        self,
        prompt: str,
        context: str,
        response: str,
        dimension: str
    ) -> Dict:
        """Evaluate a single dimension."""
        
        system_prompt = f"""You are an expert evaluator assessing AI assistant responses.
Your task is to score the response on the dimension: {dimension.upper()}

Scoring Scale (1-5):
5 = Excellent - Exceeds expectations
4 = Good - Meets expectations with minor room for improvement
3 = Adequate - Acceptable but notable gaps
2 = Poor - Below expectations, significant issues
1 = Very Poor - Fails to meet basic requirements

Be objective and consistent. Provide specific evidence from the response."""

        user_prompt = f"""ORIGINAL PROMPT:
{prompt}

REFERENCE CONTEXT:
{context[:2000]}  # Truncate for token limits

CHATBOT RESPONSE:
{response}

Evaluate this response on {dimension.upper()}.

Respond in JSON format:
{{
    "score": <integer 1-5>,
    "reasoning": "<2-3 sentences explaining the score>",
    "evidence": "<specific quote or observation from response>"
}}"""

        completion = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.1  # Low temperature for consistency
        )
        
        return json.loads(completion.choices[0].message.content)
```

### 4.3 Embeddings for Semantic Similarity

The Embeddings API enables robustness testing by measuring semantic similarity across prompt variants.

```python
import numpy as np
from typing import List

class RobustnessEvaluator:
    """Evaluate response consistency across prompt variants."""
    
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.embedding_model = "text-embedding-3-large"
    
    def get_embedding(self, text: str) -> List[float]:
        """Get embedding vector for text."""
        response = self.client.embeddings.create(
            model=self.embedding_model,
            input=text
        )
        return response.data[0].embedding
    
    def calculate_robustness_score(
        self,
        responses: List[str]
    ) -> float:
        """
        Calculate robustness score from multiple responses to prompt variants.
        
        High score = consistent responses across variants
        Low score = inconsistent/variable responses
        
        Returns: Score 1-5 based on average pairwise similarity
        """
        embeddings = [self.get_embedding(r) for r in responses]
        
        # Calculate pairwise cosine similarities
        similarities = []
        for i in range(len(embeddings)):
            for j in range(i + 1, len(embeddings)):
                sim = self._cosine_similarity(embeddings[i], embeddings[j])
                similarities.append(sim)
        
        avg_similarity = np.mean(similarities)
        
        # Map similarity [0.7, 1.0] to score [1, 5]
        # Below 0.7 similarity = score 1
        if avg_similarity < 0.7:
            return 1.0
        score = 1 + (avg_similarity - 0.7) / 0.3 * 4
        return min(5.0, score)
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """Compute cosine similarity between two vectors."""
        a, b = np.array(a), np.array(b)
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

### 4.4 API Configuration Best Practices

| Parameter | Recommended Value | Rationale |
|-----------|-------------------|-----------|
| `temperature` | 0.1 | Low variance for consistent scoring |
| `max_tokens` | 500 | Sufficient for score + reasoning |
| `response_format` | `json_object` | Structured output parsing |
| `timeout` | 60s | Account for complex evaluations |
| `retry_count` | 3 | Handle transient API errors |

---

## 5. Implementation Workflow

### 5.1 End-to-End Process Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Collect Prompts and Expected Behaviors                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Input:  BizchatIntent.csv (73 intent categories)               â”‚
â”‚  Output: synthetic_prompts_v4_enhanced_*.csv (2,640 prompts)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Automate UI Interactions                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Tool:   chatbot_stealth_agent.py                               â”‚
â”‚  Action: Submit prompts to Copilot, ChatGPT, Gemini             â”‚
â”‚  Output: Full text responses + screenshots per prompt           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Invoke OpenAI API for Judgments                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  API:    GPT-4-turbo Chat Completions                           â”‚
â”‚  Action: Score each response on 7 dimensions                    â”‚
â”‚  Output: Dimension scores with reasoning                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Compute Scores and Aggregate                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Method: Trimmed median + weighted geometric mean               â”‚
â”‚  Output: Overall UX scores per chatbot per intent               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Generate Dashboards                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Output: Competitive leaderboards, gap analysis, trends         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Step 1: Collect Prompts and Expected Behaviors

#### Input: OCV User Feedback Data

The framework starts with `BizchatIntent.csv` containing 73 intent categories extracted from Office Customer Voice (OCV) surveys:

```csv
Action,Object,Subject,Output,%_Users,UserCount,SatRate,FeedbackCount,ThumbsUpCountClipped,ThumbsDownCountClipped
FIND,EMAIL,EMAIL,,11%,128118,39%,1192,464,728
SUMMARIZE,DOCUMENT,,,7%,83291,52%,892,456,436
CREATE,EMAIL,,,5%,61234,45%,634,285,349
```

#### Processing: Synthetic Prompt Generation

The `synthetic_prompt_generator.py` transforms each intent into multiple synthetic prompts:

```python
# From synthetic_prompt_generator.py
class PromptComplexity(Enum):
    SIMPLE = "simple"      # Direct, single-step request
    MEDIUM = "medium"      # Requires some context
    COMPLEX = "complex"    # Multi-step or conditional
    EXPERT = "expert"      # Executive-level, strategic

class VariantType(Enum):
    DIRECT_REQUEST = "direct_request"
    CONTEXTUAL = "contextual"
    CONDITIONAL = "conditional"
    MULTI_STEP = "multi_step"
    EXECUTIVE_BRIEFING = "executive_briefing"
    # ... 10 variant types total
```

#### Output: Enhanced Synthetic Prompts

Each row in `synthetic_prompts_v4_enhanced_*.csv` contains:

| Field | Example |
|-------|---------|
| `synthetic_prompt_id` | SP_000001 |
| `source_intent` | GET + UPDATES |
| `synthetic_prompt` | "I'm preparing a briefing for Key Clients about Remote Work Infrastructure..." |
| `prompt_scenario` | Executive Briefing for Key Clients |
| `complexity_level` | expert |
| `context_url` | https://www.nber.org/system/files/working_papers/w30292/w30292.pdf |
| `expected_response_criteria` | ["Response should be executive-appropriate in tone..."] |
| `evaluation_dimensions` | ["accuracy", "clarity", "strategic_relevance"] |

### 5.3 Step 2: Automate UI Interactions

The stealth browser agent handles multi-chatbot automation:

```python
# Command to run evaluation on top N prompts
python chatbot_stealth_agent.py 100

# POC execution stats:
# - 100 prompts Ã— 3 chatbots = 300 evaluations
# - 203 successful responses (67.7%)
# - 645 screenshots captured
# - 1 bot detection (retried successfully)
```

#### Captured Data per Prompt

| Field | Description |
|-------|-------------|
| `response_copilot` | Full text response from Copilot |
| `response_chatgpt` | Full text response from ChatGPT |
| `response_gemini` | Full text response from Gemini |
| `screenshots_copilot` | Semicolon-delimited paths to screenshot PNGs |
| `response_time_seconds_*` | Latency per chatbot |
| `bot_detected_*` | Boolean flag for bot detection events |

### 5.4 Step 3-5: Evaluation Pipeline (To Be Implemented)

The POC has completed Steps 1-2. Steps 3-5 represent the production implementation:

```python
# Proposed evaluation pipeline
async def run_full_evaluation(results_csv: str):
    evaluator = ResponseEvaluator(api_key=os.environ["OPENAI_API_KEY"])
    
    for row in read_csv(results_csv):
        for chatbot in ["copilot", "chatgpt", "gemini"]:
            response = row[f"response_{chatbot}"]
            if not response:
                continue
            
            # Text-based evaluation
            scores = await evaluator.evaluate_response(
                prompt=row["synthetic_prompt"],
                context=row["context_text"],
                response=response,
                dimensions=json.loads(row["evaluation_dimensions"])
            )
            
            # Screenshot-based evaluation (Section 8)
            visual_scores = await evaluate_screenshots(
                screenshots=row[f"screenshots_{chatbot}"].split(";")
            )
            
            # Aggregate and store
            save_scores(row["synthetic_prompt_id"], chatbot, scores, visual_scores)
```

---

## 6. Low Satisfaction Intent Analysis

### 6.1 Identifying Problem Areas from OCV Data

The framework prioritizes intents with low user satisfaction rates from `BizchatIntent.csv`. These represent areas where Microsoft Copilot is underperforming based on real user feedback.

#### Critical Intents (0% Satisfaction Rate)

The following 20 intents have **zero positive user feedback** and represent the highest priority for improvement:

| Action | Object | Sat Rate | Thumbs Down | Priority |
|--------|--------|----------|-------------|----------|
| SHOW | FILE | 0% | 141 | ğŸ”´ Critical |
| WRITE | RESPONSE | 0% | 150 | ğŸ”´ Critical |
| PROVIDE | URL | 0% | 153 | ğŸ”´ Critical |
| LOCATE | FILE | 0% | 240 | ğŸ”´ Critical |
| ANSWER | EMAIL | 0% | 141 | ğŸ”´ Critical |
| GET | EMAIL | 0% | 52 | ğŸ”´ Critical |
| TRANSLATE | FILE | 0% | 20 | ğŸ”´ Critical |
| FIX | CODE | 0% | 44 | ğŸ”´ Critical |
| SEARCH | FILE | 0% | 44 | ğŸ”´ Critical |
| CREATE | MESSAGES | 0% | 44 | ğŸ”´ Critical |
| CREATE | PHOTO | 0% | 44 | ğŸ”´ Critical |
| IDENTIFY | PEOPLE | 0% | 52 | ğŸ”´ Critical |
| FIND | DATES | 0% | 20 | ğŸ”´ Critical |
| CREATE | NOTES | 0% | 28 | ğŸ”´ Critical |
| REDUCE | TEXT | 0% | 44 | ğŸ”´ Critical |
| LOOK | EMAIL | 0% | 44 | ğŸ”´ Critical |
| EMAIL | EMAIL | 0% | 44 | ğŸ”´ Critical |
| SHOW | CODE | 0% | 44 | ğŸ”´ Critical |
| LOCATE | EMAIL | 0% | 20 | ğŸ”´ Critical |
| REWORD | CODE | 0% | 20 | ğŸ”´ Critical |

### 6.2 Problem Category Analysis

Grouping the low-satisfaction intents reveals systemic capability gaps:

#### File Operations (38% of critical intents)
- SHOW FILE, LOCATE FILE, SEARCH FILE, TRANSLATE FILE
- **Root Cause Hypothesis:** Copilot cannot access user's file system directly
- **Competitor Advantage:** ChatGPT/Gemini may handle file-related queries more gracefully with appropriate caveats

#### Email Operations (24% of critical intents)
- GET EMAIL, ANSWER EMAIL, LOOK EMAIL, LOCATE EMAIL, EMAIL EMAIL
- **Root Cause Hypothesis:** Email integration limitations or permission issues
- **Competitor Advantage:** May provide better guidance on email workflows

#### Code Operations (14% of critical intents)
- FIX CODE, SHOW CODE, REWORD CODE
- **Root Cause Hypothesis:** Code assistance may not match user expectations set by GitHub Copilot
- **Competitor Advantage:** ChatGPT has strong code assistance reputation

#### Content Creation (24% of critical intents)
- WRITE RESPONSE, CREATE MESSAGES, CREATE PHOTO, CREATE NOTES, REDUCE TEXT
- **Root Cause Hypothesis:** Creative output quality or format issues
- **Competitor Advantage:** Need to benchmark output quality

### 6.3 Synthetic Prompt Distribution

The POC generated synthetic prompts targeting these problem areas. Distribution in `synthetic_prompts_v4_enhanced_*.csv`:

```
Source Intent Distribution (Top 20):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FIND + TICKETS           20 prompts
REPLACE + TEXT           20 prompts
INCLUDE + SECTION        10 prompts
EXPLAIN + ID             10 prompts
EXPLAIN + PROCESS        10 prompts
ADD + DATES              10 prompts
ASK + COMPANIES          10 prompts
SUMMARIZE + CONTENT      10 prompts
CREATE + FOLDER          10 prompts
FIND + AGREEMENTS        10 prompts
... (2,640 total prompts)
```

### 6.4 Evaluation Strategy for Low-Sat Intents

For critical intents, the framework applies enhanced scrutiny:

1. **Increased Sample Size**: Generate 20+ variants per critical intent (vs. 10 for normal)
2. **Stricter Thresholds**: Require score â‰¥ 4.0 for passing (vs. 3.5 for normal)
3. **Comparative Focus**: Prioritize head-to-head vs. competitors on these intents
4. **Root Cause Tagging**: Classify failures by cause (capability gap, hallucination, format issue)

```python
CRITICAL_INTENT_CONFIG = {
    "min_variants": 20,
    "passing_threshold": 4.0,
    "require_competitor_comparison": True,
    "failure_categories": [
        "capability_not_supported",
        "hallucinated_response", 
        "format_mismatch",
        "incomplete_response",
        "incorrect_information"
    ]
}
```

---

## 7. Competitive Benchmark Results

### 7.1 POC Execution Summary

The proof-of-concept evaluated **100 synthetic prompts** across three chatbots, generating comparative data for analysis:

| Metric | Copilot | ChatGPT | Gemini |
|--------|---------|---------|--------|
| **Prompts Tested** | 100 | 100 | 100 |
| **Successful Responses** | 77 | 49* | 77 |
| **Success Rate** | 77% | 49% | 77% |
| **Avg Response Time (sec)** | 14.99 | 0.57 | 8.67 |
| **Min Response Time (sec)** | 0.24 | 0.00 | 3.92 |
| **Max Response Time (sec)** | 61.60 | 9.83 | 18.20 |
| **Screenshots Captured** | ~308 | ~147 | ~190 |
| **Bot Detections** | 1 | 0 | 1 |

*Note: ChatGPT showed rate limit modal issues (`modal-no-auth-rate-limit`) affecting success rate.

### 7.2 Response Length Comparison

Response verbosity varies significantly across chatbots. Longer responses may indicate more comprehensive answers or unnecessary verbosity:

| Metric | Copilot | ChatGPT | Gemini |
|--------|---------|---------|--------|
| **Avg Response Length (chars)** | 3,752 | 2,847 | 3,289 |
| **Min Response Length** | 55 | 1,470 | 1,744 |
| **Max Response Length** | 12,692 | 5,684 | 4,374 |

#### Sample Response Length by Prompt (First 10 Prompts)

| Prompt ID | Intent | Copilot | ChatGPT | Gemini |
|-----------|--------|---------|---------|--------|
| SP_000001 | GET + UPDATES | 4,092 | 4,046 | 3,135 |
| SP_000002 | GET + UPDATES | 7,283 | 5,667 | 3,668 |
| SP_000003 | GET + UPDATES | 4,518 | 4,322 | 3,366 |
| SP_000004 | GET + UPDATES | 548 | 5,151 | 3,293 |
| SP_000005 | GET + UPDATES | 3,660 | 5,684 | 3,271 |
| SP_000006 | GET + UPDATES | 5,074 | 1,470 | 3,830 |
| SP_000007 | GET + UPDATES | 5,057 | 1,470 | 1,744 |
| SP_000008 | GET + UPDATES | 4,999 | 1,470 | 3,904 |
| SP_000009 | GET + UPDATES | 11,560 | 1,470 | 4,374 |
| SP_000010 | GET + UPDATES | 55 | 1,470 | 3,665 |

**Observation:** SP_000010 shows Copilot returning only 55 characters while competitors provided 1,470-3,665 charactersâ€”indicating a potential capability gap on this prompt type.

### 7.3 Latency Analysis

Response time is a critical UX factor. Lower latency generally correlates with better user satisfaction:

```
Latency Distribution (seconds):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Copilot:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 14.99 avg
Gemini:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8.67 avg  
ChatGPT:  â–ˆ 0.57 avg
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Key Insight:** ChatGPT demonstrates significantly faster response times (0.57s avg), though this may reflect rate-limited/error responses. Gemini provides a balance of speed and completeness.

### 7.4 Real Response Comparison: SP_000001

**Prompt:** *"I'm preparing a briefing for the Key Clients about Remote Work Infrastructure. Can you get the key updates that I should highlight? Focus on strategic impact and business outcomes."*

**Context:** NBER Working Paper on Hybrid Work (w30292.pdf)

#### Copilot Response Highlights (4,092 chars)
- âœ… Structured with emoji headers (ğŸ”‘, ğŸ“ˆ, ğŸ§­)
- âœ… 7 strategic updates with business outcomes
- âœ… Offers to create slide deck or one-page briefing
- âš ï¸ Response time: 17.72 seconds

#### ChatGPT Response Highlights (4,046 chars)
- âœ… Well-organized with 5 numbered sections
- âœ… Includes source citations [NBER]
- âœ… Strategic takeaways section
- âœ… Response time: 5.89 seconds

#### Gemini Response Highlights (3,135 chars)
- âœ… Executive-focused with 4 key findings
- âœ… Includes markdown table for impact areas
- âœ… Strategic recommendation at end
- âš ï¸ Response time: 13.67 seconds
- âš ï¸ Includes app promotion text at end

### 7.5 Complexity Distribution in POC

The POC tested prompts across complexity levels:

| Complexity | Count | Percentage |
|------------|-------|------------|
| **Expert** | 30 | 39% |
| **Complex** | 31 | 40% |
| **Medium** | 16 | 21% |
| **Simple** | 0 | 0% |

**Note:** The POC focused on higher-complexity prompts where differentiation between chatbots is more apparent.

---

## 8. Screenshot-Based Visual Evaluation

### 8.1 Novel Visual Assessment Methodology

This framework introduces a **dual-mode evaluation** approach that analyzes both text responses AND visual screenshots. This is critical because:

1. **Formatting quality** is often lost in text extraction
2. **Tables, charts, and code blocks** render differently across chatbots
3. **Visual hierarchy** (headers, spacing, emphasis) affects readability
4. **Citations and links** may display as clickable elements vs. raw text

### 8.2 Screenshot Capture Implementation

The POC captured **645 screenshots** across 100 prompts using scroll-based capture:

```
Screenshot Directory Structure:
evaluation_results/screenshots/
â”œâ”€â”€ SP_000001/
â”‚   â”œâ”€â”€ copilot/
â”‚   â”‚   â”œâ”€â”€ response_1_183324.png  (213 KB)
â”‚   â”‚   â”œâ”€â”€ response_2_183324.png  (173 KB)
â”‚   â”‚   â”œâ”€â”€ response_3_183325.png  (167 KB)
â”‚   â”‚   â””â”€â”€ response_4_183325.png  (150 KB)
â”‚   â”œâ”€â”€ chatgpt/
â”‚   â”‚   â”œâ”€â”€ response_1_183400.png
â”‚   â”‚   â”œâ”€â”€ response_2_183401.png
â”‚   â”‚   â””â”€â”€ response_3_183402.png
â”‚   â””â”€â”€ gemini/
â”‚       â”œâ”€â”€ response_1_183444.png
â”‚       â””â”€â”€ response_2_183444.png
â”œâ”€â”€ SP_000002/
â”‚   â””â”€â”€ ...
â””â”€â”€ SP_000100/
    â””â”€â”€ ...
```

### 8.3 Visual Evaluation Dimensions

Screenshots enable assessment of dimensions that text analysis cannot capture:

| Dimension | What to Evaluate | Scoring Criteria |
|-----------|------------------|------------------|
| **Visual Hierarchy** | Headers, subheaders, logical flow | Are sections clearly delineated? |
| **Table Rendering** | Data tables, comparison charts | Are tables properly aligned and readable? |
| **Code Formatting** | Syntax highlighting, line numbers | Is code in proper blocks with highlighting? |
| **Whitespace Usage** | Paragraph spacing, breathing room | Is content scannable or dense? |
| **Citation Display** | Source links, reference styling | Are citations clickable and visible? |
| **Emoji/Icon Usage** | Visual markers, bullet styles | Do visual elements enhance or clutter? |
| **Mobile Readability** | Content width, text size | Would this render well on smaller screens? |

### 8.4 Vision Model Integration (Implementation Guide)

To evaluate screenshots programmatically, integrate OpenAI's GPT-4 Vision or similar multimodal models:

```python
import base64
import openai
from pathlib import Path
from typing import List, Dict

class VisualEvaluator:
    """Evaluate chatbot responses using screenshot analysis."""
    
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = "gpt-4-vision-preview"
    
    def encode_image(self, image_path: str) -> str:
        """Encode image to base64 for API submission."""
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode("utf-8")
    
    async def evaluate_visual_response(
        self,
        screenshot_paths: List[str],
        prompt: str,
        chatbot: str
    ) -> Dict:
        """
        Evaluate visual quality of a chatbot response from screenshots.
        
        Args:
            screenshot_paths: List of paths to response screenshots
            prompt: The original prompt for context
            chatbot: Name of the chatbot (copilot/chatgpt/gemini)
        
        Returns:
            Dict with visual dimension scores and observations
        """
        # Encode all screenshots
        images = []
        for path in screenshot_paths[:4]:  # Limit to 4 for token efficiency
            if Path(path).exists():
                images.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{self.encode_image(path)}",
                        "detail": "high"
                    }
                })
        
        if not images:
            return {"error": "No valid screenshots found"}
        
        system_prompt = """You are an expert UX evaluator analyzing AI chatbot response screenshots.

Evaluate the visual presentation on these dimensions (score 1-5 each):

1. VISUAL_HIERARCHY: Are headers, sections, and content levels clear?
2. TABLE_RENDERING: Are any tables properly formatted and readable?
3. CODE_FORMATTING: Is code properly highlighted and in code blocks?
4. WHITESPACE: Is spacing appropriate for readability?
5. CITATION_DISPLAY: Are sources/links clearly visible and usable?
6. OVERALL_FORMATTING: General visual quality and professionalism

Respond in JSON format with scores and specific observations."""

        user_content = [
            {"type": "text", "text": f"Original prompt: {prompt}\n\nChatbot: {chatbot}\n\nEvaluate these response screenshots:"}
        ] + images
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            response_format={"type": "json_object"},
            max_tokens=1000
        )
        
        return json.loads(response.choices[0].message.content)
    
    async def compare_visual_responses(
        self,
        prompt_id: str,
        prompt: str,
        screenshots_dir: Path
    ) -> Dict:
        """
        Compare visual quality across all chatbots for a single prompt.
        
        Returns comparative analysis with winner per dimension.
        """
        results = {}
        
        for chatbot in ["copilot", "chatgpt", "gemini"]:
            chatbot_dir = screenshots_dir / prompt_id / chatbot
            if chatbot_dir.exists():
                screenshots = sorted(chatbot_dir.glob("*.png"))
                if screenshots:
                    results[chatbot] = await self.evaluate_visual_response(
                        [str(s) for s in screenshots],
                        prompt,
                        chatbot
                    )
        
        # Determine winner per dimension
        dimensions = ["VISUAL_HIERARCHY", "TABLE_RENDERING", "CODE_FORMATTING", 
                      "WHITESPACE", "CITATION_DISPLAY", "OVERALL_FORMATTING"]
        
        comparison = {"scores": results, "winners": {}}
        for dim in dimensions:
            scores = {cb: results.get(cb, {}).get(dim, 0) for cb in results}
            if scores:
                comparison["winners"][dim] = max(scores, key=scores.get)
        
        return comparison
```

### 8.5 Visual Evaluation Insights (Expected Findings)

Based on manual review of captured screenshots, anticipated comparative insights:

| Dimension | Expected Leader | Observation |
|-----------|-----------------|-------------|
| **Visual Hierarchy** | ChatGPT | Clean section headers, consistent styling |
| **Table Rendering** | Gemini | Native markdown table support |
| **Code Formatting** | ChatGPT | Strong syntax highlighting |
| **Whitespace** | Copilot | Good spacing, emoji usage |
| **Citation Display** | Copilot | Clickable source cards |
| **Overall** | TBD | Requires full evaluation run |

---

## 9. Text Response Evaluation Framework

### 9.1 LLM-as-Judge Architecture

The text evaluation framework uses GPT-4 as an automated judge to score responses across the dimensions defined in Section 2.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Text Response Evaluation                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Prompt     â”‚    â”‚   Context    â”‚    â”‚   Response   â”‚      â”‚
â”‚  â”‚              â”‚    â”‚   (URL/Doc)  â”‚    â”‚   (Text)     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â”‚                                    â”‚
â”‚                             â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   GPT-4 Judge                             â”‚   â”‚
â”‚  â”‚                                                           â”‚   â”‚
â”‚  â”‚  For each dimension:                                      â”‚   â”‚
â”‚  â”‚  â€¢ Analyze response against criteria                      â”‚   â”‚
â”‚  â”‚  â€¢ Assign score (1-5)                                     â”‚   â”‚
â”‚  â”‚  â€¢ Provide reasoning and evidence                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                    â”‚
â”‚                             â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Output per Response:                                     â”‚   â”‚
â”‚  â”‚  {                                                        â”‚   â”‚
â”‚  â”‚    "factuality": {"score": 4, "reasoning": "..."},       â”‚   â”‚
â”‚  â”‚    "helpfulness": {"score": 5, "reasoning": "..."},      â”‚   â”‚
â”‚  â”‚    "formatting": {"score": 3, "reasoning": "..."},       â”‚   â”‚
â”‚  â”‚    ...                                                    â”‚   â”‚
â”‚  â”‚  }                                                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 Evaluation Prompt Templates

#### Factuality Evaluation

```python
FACTUALITY_PROMPT = """
You are evaluating the FACTUALITY of an AI assistant's response.

ORIGINAL USER PROMPT:
{prompt}

REFERENCE CONTEXT (ground truth):
{context}

AI RESPONSE TO EVALUATE:
{response}

FACTUALITY measures whether claims in the response are:
- Accurate and verifiable from the provided context
- Free from hallucinations or fabricated information
- Properly attributed to sources when making claims

Score 1-5:
5 = All claims verified, no hallucinations, proper citations
4 = Minor unsupported claims, core facts accurate
3 = Mix of accurate and questionable claims
2 = Significant factual errors or unsupported claims
1 = Predominantly fabricated or incorrect information

Respond in JSON:
{
    "score": <1-5>,
    "verified_claims": ["<claim that matches context>", ...],
    "questionable_claims": ["<claim not in context>", ...],
    "hallucinations": ["<fabricated information>", ...],
    "reasoning": "<2-3 sentence explanation>"
}
"""
```

#### Helpfulness Evaluation

```python
HELPFULNESS_PROMPT = """
You are evaluating the HELPFULNESS of an AI assistant's response.

ORIGINAL USER PROMPT:
{prompt}

EXPECTED RESPONSE CRITERIA:
{expected_criteria}

AI RESPONSE TO EVALUATE:
{response}

HELPFULNESS measures whether the response:
- Directly addresses the user's stated need
- Provides actionable information or next steps
- Is complete enough for the user to proceed
- Anticipates follow-up needs

Score 1-5:
5 = Fully addresses need, actionable, complete, anticipatory
4 = Addresses most needs with minor gaps
3 = Partially helpful, requires clarification
2 = Minimally helpful, misses key aspects
1 = Does not address user need

Respond in JSON:
{
    "score": <1-5>,
    "needs_addressed": ["<user need met>", ...],
    "needs_missed": ["<user need not addressed>", ...],
    "actionability": "<high/medium/low>",
    "reasoning": "<2-3 sentence explanation>"
}
"""
```

### 9.3 Batch Evaluation Pipeline

```python
import asyncio
from typing import List, Dict
import pandas as pd

class BatchTextEvaluator:
    """Evaluate multiple responses in parallel for efficiency."""
    
    def __init__(self, api_key: str, max_concurrent: int = 5):
        self.evaluator = ResponseEvaluator(api_key)
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def evaluate_batch(
        self,
        results_df: pd.DataFrame,
        dimensions: List[str] = ["factuality", "helpfulness", "formatting"]
    ) -> pd.DataFrame:
        """
        Evaluate all responses in a results dataframe.
        
        Args:
            results_df: DataFrame with columns for each chatbot's response
            dimensions: Which dimensions to evaluate
        
        Returns:
            DataFrame with added score columns
        """
        tasks = []
        
        for idx, row in results_df.iterrows():
            for chatbot in ["copilot", "chatgpt", "gemini"]:
                response = row.get(f"response_{chatbot}", "")
                if response and len(response) > 50:
                    task = self._evaluate_with_limit(
                        prompt_id=row["synthetic_prompt_id"],
                        chatbot=chatbot,
                        prompt=row["synthetic_prompt"],
                        context=row.get("context_text", ""),
                        response=response,
                        dimensions=dimensions
                    )
                    tasks.append(task)
        
        # Execute with rate limiting
        scores = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Merge scores back to dataframe
        return self._merge_scores(results_df, scores)
    
    async def _evaluate_with_limit(self, **kwargs) -> Dict:
        """Apply concurrency limit to API calls."""
        async with self.semaphore:
            return await self.evaluator.evaluate_response(
                prompt=kwargs["prompt"],
                context=kwargs["context"],
                response=kwargs["response"],
                dimensions=kwargs["dimensions"]
            )
```

### 9.4 Expected Output Schema

After running the text evaluation pipeline, the output CSV gains these columns:

| Column | Type | Description |
|--------|------|-------------|
| `score_factuality_copilot` | float | Factuality score 1-5 for Copilot |
| `score_factuality_chatgpt` | float | Factuality score 1-5 for ChatGPT |
| `score_factuality_gemini` | float | Factuality score 1-5 for Gemini |
| `score_helpfulness_copilot` | float | Helpfulness score 1-5 |
| `score_formatting_copilot` | float | Formatting score 1-5 |
| `reasoning_factuality_copilot` | str | Judge's reasoning |
| `overall_score_copilot` | float | Weighted geometric mean |
| `winner_factuality` | str | Best performer on this dimension |
| `winner_overall` | str | Best overall performer |

### 9.5 Comparative Analysis Queries

After evaluation, analysts can query for competitive insights:

```python
# Find prompts where Copilot underperforms
copilot_gaps = df[
    (df["score_overall_copilot"] < df["score_overall_chatgpt"]) |
    (df["score_overall_copilot"] < df["score_overall_gemini"])
]

# Identify worst dimensions for Copilot
dimension_scores = df[[
    "score_factuality_copilot",
    "score_helpfulness_copilot", 
    "score_formatting_copilot"
]].mean()
worst_dimension = dimension_scores.idxmin()

# Group gaps by intent category
gaps_by_intent = copilot_gaps.groupby("source_intent").agg({
    "synthetic_prompt_id": "count",
    "score_overall_copilot": "mean",
    "score_overall_chatgpt": "mean"
}).sort_values("synthetic_prompt_id", ascending=False)
```

---

## 10. Bias Mitigation and Calibration

### 10.1 Sources of Evaluation Bias

LLM-as-judge systems can exhibit systematic biases that compromise evaluation validity:

| Bias Type | Description | Risk Level |
|-----------|-------------|------------|
| **Verbosity Bias** | Preference for longer responses regardless of quality | High |
| **Position Bias** | Favoring responses presented first in comparisons | Medium |
| **Self-Preference** | GPT-4 may favor GPT-4-generated responses (ChatGPT) | High |
| **Style Bias** | Preference for certain formatting patterns | Medium |
| **Anchoring** | Over-reliance on first impressions or examples | Medium |

### 10.2 Judge Ensemble Diversity

To mitigate single-judge bias, the framework supports **multi-judge ensembles**:

```python
class JudgeEnsemble:
    """Multiple LLM judges for bias reduction."""
    
    JUDGE_CONFIGS = {
        "gpt4_turbo": {
            "model": "gpt-4-turbo",
            "provider": "openai",
            "temperature": 0.1
        },
        "gpt4o": {
            "model": "gpt-4o",
            "provider": "openai", 
            "temperature": 0.1
        },
        "claude_sonnet": {
            "model": "claude-3-5-sonnet-20241022",
            "provider": "anthropic",
            "temperature": 0.1
        }
    }
    
    def __init__(self, judges: List[str] = None):
        self.judges = judges or ["gpt4_turbo", "claude_sonnet"]
        self.clients = self._initialize_clients()
    
    async def evaluate_with_ensemble(
        self,
        prompt: str,
        context: str,
        response: str,
        dimension: str
    ) -> Dict:
        """Get scores from multiple judges and aggregate."""
        
        scores = []
        reasonings = []
        
        for judge_name in self.judges:
            result = await self._evaluate_with_judge(
                judge_name, prompt, context, response, dimension
            )
            scores.append(result["score"])
            reasonings.append(result["reasoning"])
        
        return {
            "ensemble_score": np.median(scores),
            "score_variance": np.var(scores),
            "individual_scores": dict(zip(self.judges, scores)),
            "agreement_rate": self._calculate_agreement(scores),
            "reasonings": dict(zip(self.judges, reasonings))
        }
    
    def _calculate_agreement(self, scores: List[float]) -> float:
        """Calculate inter-judge agreement (% within 1 point)."""
        agreements = 0
        comparisons = 0
        for i in range(len(scores)):
            for j in range(i + 1, len(scores)):
                comparisons += 1
                if abs(scores[i] - scores[j]) <= 1:
                    agreements += 1
        return agreements / comparisons if comparisons > 0 else 1.0
```

### 10.3 Calibration Against Human-Rated Gold Sets

To ensure LLM judges align with human judgment, maintain a **gold set** of human-rated responses:

#### Gold Set Requirements

| Criterion | Specification |
|-----------|---------------|
| **Size** | Minimum 100 prompt-response pairs |
| **Coverage** | All 7 evaluation dimensions represented |
| **Raters** | 3+ human raters per item |
| **Diversity** | Include all complexity levels and intent types |
| **Refresh Rate** | Update quarterly with new examples |

#### Calibration Process

```python
class JudgeCalibrator:
    """Calibrate LLM judges against human gold set."""
    
    def __init__(self, gold_set_path: str):
        self.gold_set = pd.read_csv(gold_set_path)
        self.calibration_results = {}
    
    def calibrate_judge(self, judge: ResponseEvaluator) -> Dict:
        """
        Run judge on gold set and compute alignment metrics.
        
        Returns:
            Dict with correlation, MAE, and bias metrics
        """
        predictions = []
        human_scores = []
        
        for _, row in self.gold_set.iterrows():
            pred = judge.evaluate_response(
                row["prompt"], row["context"], row["response"],
                [row["dimension"]]
            )
            predictions.append(pred[row["dimension"]]["score"])
            human_scores.append(row["human_score"])
        
        return {
            "pearson_correlation": np.corrcoef(predictions, human_scores)[0, 1],
            "spearman_correlation": stats.spearmanr(predictions, human_scores)[0],
            "mean_absolute_error": np.mean(np.abs(np.array(predictions) - np.array(human_scores))),
            "bias": np.mean(np.array(predictions) - np.array(human_scores)),  # Positive = inflated
            "agreement_within_1": np.mean(np.abs(np.array(predictions) - np.array(human_scores)) <= 1)
        }
    
    def generate_calibration_report(self) -> str:
        """Generate human-readable calibration report."""
        report = "# Judge Calibration Report\n\n"
        
        for judge_name, metrics in self.calibration_results.items():
            report += f"## {judge_name}\n"
            report += f"- Pearson r: {metrics['pearson_correlation']:.3f}\n"
            report += f"- MAE: {metrics['mean_absolute_error']:.2f}\n"
            report += f"- Bias: {metrics['bias']:+.2f}\n"
            report += f"- Agreement (Â±1): {metrics['agreement_within_1']:.1%}\n\n"
        
        return report
```

#### Calibration Thresholds

| Metric | Acceptable | Good | Excellent |
|--------|------------|------|-----------|
| **Pearson Correlation** | â‰¥ 0.60 | â‰¥ 0.75 | â‰¥ 0.85 |
| **Mean Absolute Error** | â‰¤ 1.0 | â‰¤ 0.7 | â‰¤ 0.5 |
| **Agreement (Â±1 point)** | â‰¥ 70% | â‰¥ 80% | â‰¥ 90% |
| **Bias** | Â±0.5 | Â±0.3 | Â±0.1 |

### 10.4 Blind Evaluation Techniques

To prevent bias from knowing which chatbot generated a response:

#### Implementation

```python
class BlindEvaluator:
    """Evaluate responses without revealing source chatbot."""
    
    def prepare_blind_batch(self, results_df: pd.DataFrame) -> pd.DataFrame:
        """
        Prepare responses for blind evaluation.
        
        - Removes chatbot identifiers
        - Strips chatbot-specific artifacts
        - Randomizes presentation order
        """
        blind_rows = []
        
        for _, row in results_df.iterrows():
            responses = []
            for chatbot in ["copilot", "chatgpt", "gemini"]:
                response = row.get(f"response_{chatbot}", "")
                if response:
                    # Strip identifying artifacts
                    cleaned = self._remove_identifiers(response, chatbot)
                    responses.append({
                        "prompt_id": row["synthetic_prompt_id"],
                        "response": cleaned,
                        "_source": chatbot  # Hidden from judge
                    })
            
            # Randomize order
            random.shuffle(responses)
            
            # Assign anonymous labels
            for i, r in enumerate(responses):
                r["label"] = f"Response_{chr(65 + i)}"  # A, B, C
                blind_rows.append(r)
        
        return pd.DataFrame(blind_rows)
    
    def _remove_identifiers(self, response: str, chatbot: str) -> str:
        """Remove chatbot-specific text patterns."""
        patterns = {
            "copilot": [
                r"Show all$",
                r"www\.\w+\.com$",
                r"\[.*?\]\(https://.*?\)"  # Copilot citation cards
            ],
            "chatgpt": [
                r"\ã€\d+â€ source\ã€‘",  # ChatGPT citations
            ],
            "gemini": [
                r"By the way, to unlock.*?Gemini Apps Activity\.",
                r"Would you like me to.*?\?"  # Trailing offers
            ]
        }
        
        cleaned = response
        for pattern in patterns.get(chatbot, []):
            cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE)
        
        return cleaned.strip()
```

### 10.5 Bias Detection and Monitoring

Continuously monitor for emerging biases:

```python
class BiasMonitor:
    """Detect systematic evaluation biases."""
    
    def detect_verbosity_bias(self, results_df: pd.DataFrame) -> Dict:
        """Check if scores correlate with response length."""
        correlations = {}
        
        for chatbot in ["copilot", "chatgpt", "gemini"]:
            lengths = results_df[f"response_{chatbot}"].str.len()
            scores = results_df[f"score_overall_{chatbot}"]
            
            valid = ~(lengths.isna() | scores.isna())
            if valid.sum() > 10:
                corr = np.corrcoef(lengths[valid], scores[valid])[0, 1]
                correlations[chatbot] = corr
        
        return {
            "correlations": correlations,
            "bias_detected": any(abs(c) > 0.3 for c in correlations.values()),
            "recommendation": "Apply length normalization" if any(abs(c) > 0.3 for c in correlations.values()) else "No action needed"
        }
    
    def detect_position_bias(self, blind_results: pd.DataFrame) -> Dict:
        """Check if first-presented responses score higher."""
        position_scores = blind_results.groupby("label")["score"].mean()
        
        # Response_A presented first, should not consistently score higher
        first_advantage = position_scores.get("Response_A", 0) - position_scores.mean()
        
        return {
            "position_scores": position_scores.to_dict(),
            "first_position_advantage": first_advantage,
            "bias_detected": abs(first_advantage) > 0.3
        }
```

---

## 11. Reporting and Visualization

### 11.1 Competitive Leaderboard Structure

The primary output is a leaderboard combining dimension breakdowns and aggregate scores:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    COMPETITIVE EVALUATION LEADERBOARD                         â•‘
â•‘                    Evaluation Date: January 13, 2026                          â•‘
â•‘                    Prompts Evaluated: 100 | Dimensions: 7                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  RANK â”‚ CHATBOT  â”‚ OVERALL â”‚ FACT â”‚ HELP â”‚ SAFE â”‚ ROBUST â”‚ LAT â”‚ FMT â”‚ MEM  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘   1   â”‚ ChatGPT  â”‚  4.32   â”‚ 4.5  â”‚ 4.4  â”‚ 4.8  â”‚  4.1   â”‚ 4.9 â”‚ 4.2 â”‚ 3.8  â•‘
â•‘   2   â”‚ Gemini   â”‚  4.18   â”‚ 4.3  â”‚ 4.2  â”‚ 4.7  â”‚  4.0   â”‚ 4.3 â”‚ 4.1 â”‚ 3.9  â•‘
â•‘   3   â”‚ Copilot  â”‚  3.95   â”‚ 4.1  â”‚ 4.0  â”‚ 4.6  â”‚  3.8   â”‚ 3.2 â”‚ 4.3 â”‚ 3.7  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Legend: FACT=Factuality, HELP=Helpfulness, SAFE=Safety, ROBUST=Robustness,
        LAT=Latency, FMT=Formatting, MEM=Memory
```

### 11.2 Intent-Level Gap Analysis

Drill down to specific problem intents:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              COPILOT GAP ANALYSIS BY INTENT (vs. Best Competitor)            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  INTENT              â”‚ COPILOT â”‚ BEST    â”‚ GAP   â”‚ PRIORITY â”‚ SAMPLE SIZE   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  LOCATE + FILE       â”‚  2.1    â”‚ 4.2 (G) â”‚ -2.1  â”‚ ğŸ”´ CRIT  â”‚ 10 prompts    â•‘
â•‘  SHOW + FILE         â”‚  2.3    â”‚ 4.0 (C) â”‚ -1.7  â”‚ ğŸ”´ CRIT  â”‚ 10 prompts    â•‘
â•‘  TRANSLATE + FILE    â”‚  2.5    â”‚ 4.1 (C) â”‚ -1.6  â”‚ ğŸ”´ CRIT  â”‚ 10 prompts    â•‘
â•‘  GET + EMAIL         â”‚  2.8    â”‚ 4.3 (G) â”‚ -1.5  â”‚ ğŸŸ  HIGH  â”‚ 10 prompts    â•‘
â•‘  FIX + CODE          â”‚  3.0    â”‚ 4.4 (C) â”‚ -1.4  â”‚ ğŸŸ  HIGH  â”‚ 10 prompts    â•‘
â•‘  CREATE + DOCUMENT   â”‚  3.5    â”‚ 4.2 (C) â”‚ -0.7  â”‚ ğŸŸ¡ MED   â”‚ 10 prompts    â•‘
â•‘  SUMMARIZE + EMAIL   â”‚  3.8    â”‚ 4.1 (G) â”‚ -0.3  â”‚ ğŸŸ¢ LOW   â”‚ 10 prompts    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Legend: (C)=ChatGPT leader, (G)=Gemini leader
```

### 11.3 Threshold-Based Release Gating

Define quality gates for product releases:

```python
RELEASE_GATES = {
    "production": {
        "overall_score_min": 4.0,
        "dimension_floor": 3.5,  # No dimension below this
        "safety_min": 4.5,       # Higher bar for safety
        "regression_tolerance": -0.1,  # Max allowed drop from baseline
        "competitive_gap_max": -0.5    # Max gap vs. best competitor
    },
    "preview": {
        "overall_score_min": 3.5,
        "dimension_floor": 3.0,
        "safety_min": 4.0,
        "regression_tolerance": -0.2,
        "competitive_gap_max": -1.0
    },
    "internal": {
        "overall_score_min": 3.0,
        "dimension_floor": 2.5,
        "safety_min": 3.5,
        "regression_tolerance": -0.5,
        "competitive_gap_max": None  # No gate
    }
}

def check_release_gate(scores: Dict, gate: str = "production") -> Dict:
    """
    Check if current scores pass release gate.
    
    Returns:
        Dict with pass/fail status and blocking issues
    """
    config = RELEASE_GATES[gate]
    issues = []
    
    # Check overall score
    if scores["overall"] < config["overall_score_min"]:
        issues.append(f"Overall score {scores['overall']:.2f} < {config['overall_score_min']}")
    
    # Check dimension floor
    for dim, score in scores["dimensions"].items():
        if score < config["dimension_floor"]:
            issues.append(f"{dim} score {score:.2f} < floor {config['dimension_floor']}")
    
    # Check safety minimum
    if scores["dimensions"].get("safety", 5) < config["safety_min"]:
        issues.append(f"Safety score below minimum {config['safety_min']}")
    
    # Check competitive gap
    if config["competitive_gap_max"] is not None:
        gap = scores["overall"] - scores["best_competitor"]
        if gap < config["competitive_gap_max"]:
            issues.append(f"Competitive gap {gap:.2f} exceeds threshold")
    
    return {
        "gate": gate,
        "passed": len(issues) == 0,
        "blocking_issues": issues,
        "recommendation": "Proceed to release" if len(issues) == 0 else "Address blocking issues"
    }
```

### 11.4 Dashboard Visualizations

#### Recommended Charts

| Chart Type | Purpose | Data Source |
|------------|---------|-------------|
| **Radar Chart** | Compare chatbots across all dimensions | Dimension scores |
| **Bar Chart (Grouped)** | Intent-level score comparison | Per-intent aggregates |
| **Time Series** | Track score trends over evaluation runs | Historical scores |
| **Heatmap** | Dimension Ã— Intent gap matrix | Gap analysis |
| **Box Plot** | Score distribution per chatbot | All individual scores |

#### Sample Radar Chart Data

```json
{
  "labels": ["Factuality", "Helpfulness", "Safety", "Robustness", "Latency", "Formatting", "Memory"],
  "datasets": [
    {
      "label": "Copilot",
      "data": [4.1, 4.0, 4.6, 3.8, 3.2, 4.3, 3.7],
      "borderColor": "#0078D4"
    },
    {
      "label": "ChatGPT", 
      "data": [4.5, 4.4, 4.8, 4.1, 4.9, 4.2, 3.8],
      "borderColor": "#10A37F"
    },
    {
      "label": "Gemini",
      "data": [4.3, 4.2, 4.7, 4.0, 4.3, 4.1, 3.9],
      "borderColor": "#4285F4"
    }
  ]
}
```

### 11.5 Automated Reporting Pipeline

```python
class ReportGenerator:
    """Generate automated evaluation reports."""
    
    def generate_executive_summary(self, results: pd.DataFrame) -> str:
        """One-page executive summary for leadership."""
        
        # Calculate key metrics
        copilot_avg = results["score_overall_copilot"].mean()
        chatgpt_avg = results["score_overall_chatgpt"].mean()
        gemini_avg = results["score_overall_gemini"].mean()
        
        leader = max([
            ("Copilot", copilot_avg),
            ("ChatGPT", chatgpt_avg),
            ("Gemini", gemini_avg)
        ], key=lambda x: x[1])
        
        copilot_rank = sorted([copilot_avg, chatgpt_avg, gemini_avg], reverse=True).index(copilot_avg) + 1
        
        # Find biggest gaps
        gaps = []
        for intent in results["source_intent"].unique():
            intent_data = results[results["source_intent"] == intent]
            copilot_score = intent_data["score_overall_copilot"].mean()
            best_competitor = max(
                intent_data["score_overall_chatgpt"].mean(),
                intent_data["score_overall_gemini"].mean()
            )
            gaps.append((intent, copilot_score - best_competitor))
        
        worst_gaps = sorted(gaps, key=lambda x: x[1])[:5]
        
        return f"""
# Competitive Evaluation Executive Summary
**Date:** {datetime.now().strftime('%B %d, %Y')}

## Overall Standing
- **Copilot Rank:** #{copilot_rank} of 3
- **Copilot Score:** {copilot_avg:.2f} / 5.0
- **Market Leader:** {leader[0]} ({leader[1]:.2f})

## Key Gaps Requiring Attention
{chr(10).join(f"- {intent}: {gap:+.2f} vs. best competitor" for intent, gap in worst_gaps)}

## Recommendation
{"âœ… PASS: Copilot meets competitive parity" if copilot_rank == 1 else "âš ï¸ ACTION NEEDED: Address capability gaps"}
"""
```

---

## 12. Appendix: Statistical Notes

### 12.1 Confidence Intervals

For score estimates, compute 95% confidence intervals using the standard error:

$$
CI_{95\%} = \bar{x} \pm 1.96 \times \frac{s}{\sqrt{n}}
$$

Where:
- $\bar{x}$ = sample mean score
- $s$ = sample standard deviation
- $n$ = number of evaluations

**Example Calculation (POC Data):**

For Copilot response time (n=77, mean=14.99s, estimated std=12s):

$$
CI_{95\%} = 14.99 \pm 1.96 \times \frac{12}{\sqrt{77}} = 14.99 \pm 2.68 = [12.31, 17.67]
$$

### 12.2 Bootstrap Methods

For more robust confidence intervals, especially with non-normal distributions:

```python
import numpy as np
from scipy import stats

def bootstrap_confidence_interval(
    data: np.ndarray,
    statistic: callable = np.mean,
    n_bootstrap: int = 10000,
    confidence: float = 0.95
) -> tuple:
    """
    Compute bootstrap confidence interval.
    
    Args:
        data: Array of values
        statistic: Function to compute (default: mean)
        n_bootstrap: Number of bootstrap samples
        confidence: Confidence level (default: 95%)
    
    Returns:
        (lower_bound, upper_bound, point_estimate)
    """
    n = len(data)
    bootstrap_stats = []
    
    for _ in range(n_bootstrap):
        # Resample with replacement
        sample = np.random.choice(data, size=n, replace=True)
        bootstrap_stats.append(statistic(sample))
    
    bootstrap_stats = np.array(bootstrap_stats)
    
    alpha = 1 - confidence
    lower = np.percentile(bootstrap_stats, 100 * alpha / 2)
    upper = np.percentile(bootstrap_stats, 100 * (1 - alpha / 2))
    point = statistic(data)
    
    return (lower, upper, point)

# Example usage
scores = np.array([4.2, 3.8, 4.5, 3.9, 4.1, 4.0, 3.7, 4.3])
lower, upper, mean = bootstrap_confidence_interval(scores)
print(f"Mean: {mean:.2f}, 95% CI: [{lower:.2f}, {upper:.2f}]")
```

### 12.3 Normalization Formulas

#### Min-Max Normalization

Convert raw scores to 0-1 range:

$$
x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

#### Z-Score Normalization

Standardize to mean=0, std=1:

$$
z = \frac{x - \mu}{\sigma}
$$

#### Latency to Score Conversion

Convert response time (seconds) to 1-5 score:

$$
\text{score} = \max\left(1, 5 - 4 \times \frac{\ln(t + 1)}{\ln(t_{max} + 1)}\right)
$$

This logarithmic scaling ensures:
- Fast responses (< 1s) â†’ score â‰ˆ 5
- Moderate responses (5-10s) â†’ score â‰ˆ 3-4
- Slow responses (> 30s) â†’ score â‰ˆ 1-2

### 12.4 Statistical Significance Testing

#### Paired t-test for Chatbot Comparison

When comparing scores between two chatbots on the same prompts:

```python
from scipy import stats

def compare_chatbots(
    scores_a: np.ndarray,
    scores_b: np.ndarray,
    chatbot_a: str,
    chatbot_b: str
) -> Dict:
    """
    Test if score difference is statistically significant.
    
    Uses paired t-test since same prompts evaluated.
    """
    # Remove pairs with missing data
    valid = ~(np.isnan(scores_a) | np.isnan(scores_b))
    a = scores_a[valid]
    b = scores_b[valid]
    
    # Paired t-test
    t_stat, p_value = stats.ttest_rel(a, b)
    
    # Effect size (Cohen's d)
    diff = a - b
    cohens_d = np.mean(diff) / np.std(diff)
    
    return {
        "mean_a": np.mean(a),
        "mean_b": np.mean(b),
        "mean_difference": np.mean(diff),
        "t_statistic": t_stat,
        "p_value": p_value,
        "significant_at_05": p_value < 0.05,
        "significant_at_01": p_value < 0.01,
        "cohens_d": cohens_d,
        "effect_size": "large" if abs(cohens_d) > 0.8 else "medium" if abs(cohens_d) > 0.5 else "small",
        "interpretation": f"{chatbot_a} {'>' if np.mean(diff) > 0 else '<'} {chatbot_b} (p={p_value:.4f})"
    }
```

### 12.5 Sample Size Recommendations

| Analysis Type | Minimum n | Recommended n | Notes |
|---------------|-----------|---------------|-------|
| **Overall Score Comparison** | 30 | 100+ | Per chatbot |
| **Per-Dimension Analysis** | 50 | 200+ | For reliable dimension scores |
| **Per-Intent Analysis** | 10 | 30+ | Per intent category |
| **Trend Detection** | 3 runs | 10+ runs | Historical comparison |
| **Calibration Gold Set** | 50 | 100+ | Human-rated examples |

### 12.6 Inter-Rater Reliability

When using multiple judges, measure agreement with Krippendorff's Alpha:

$$
\alpha = 1 - \frac{D_o}{D_e}
$$

Where:
- $D_o$ = observed disagreement
- $D_e$ = expected disagreement by chance

**Interpretation:**
| Alpha | Interpretation |
|-------|----------------|
| â‰¥ 0.80 | Good reliability |
| 0.67 - 0.80 | Acceptable for exploratory |
| < 0.67 | Questionable reliability |

---

## Conclusion

This technical guide provides a comprehensive framework for evaluating Microsoft Copilot against competitors using:

1. **Synthetic Prompt Generation** â€” Data-driven prompt creation from OCV user feedback
2. **Stealth Browser Automation** â€” Scalable, detection-resistant chatbot interaction
3. **Dual-Mode Evaluation** â€” Both text and visual (screenshot) analysis
4. **LLM-as-Judge Scoring** â€” Automated, consistent quality assessment
5. **Competitive Benchmarking** â€” Head-to-head comparison with ChatGPT and Gemini

### Next Steps for Production Implementation

| Phase | Scope | Timeline |
|-------|-------|----------|
| **Phase 1** | Complete text evaluation pipeline | 2 weeks |
| **Phase 2** | Implement visual (screenshot) evaluation | 3 weeks |
| **Phase 3** | Build calibration gold set | 4 weeks |
| **Phase 4** | Deploy automated reporting dashboard | 2 weeks |
| **Phase 5** | Scale to full 2,640 prompts | Ongoing |

### Contact

For questions about this framework, contact the Evaluation Framework Team.

---

*Document Version 1.0 â€” January 13, 2026*
